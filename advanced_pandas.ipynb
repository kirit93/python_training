{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAl-YipkvOtC"
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# Welcome to Module 6: Data Analysis in Pandas\n",
    "\n",
    "# -- Contents --\n",
    "# 1. DATA DESCRIPTIONS AND AGGREGATIONS\n",
    "# 2. GROUPED AGGREGATIONS\n",
    "# 3. SORTING AND FILTERING\n",
    "# 4. TABLE JOINS\n",
    "# 4. ADVANCED STATISTICAL METHODS\n",
    "# 5. TRANSFORMING DATAFRAMES\n",
    "# 6. VISUALIZING DATA\n",
    "# 7. ACTIVITIES\n",
    "\n",
    "################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bu4oSL5B5Xpt"
   },
   "source": [
    "\n",
    "########################\n",
    "# 1. DATA DESCRIPTIONS AND AGGREGATIONS\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xW_W28pf5bGY"
   },
   "source": [
    "##########\n",
    "## 1.1 SETUP\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXHlZ23O5SWH"
   },
   "outputs": [],
   "source": [
    "# First, we'll import several libraries\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "id": "8X8RV_zJ5e97",
    "outputId": "756f5bea-02cf-4100-afda-f2c24b8de151"
   },
   "outputs": [],
   "source": [
    "# Then, we'll load the 2 dataframes that we'll be working with\n",
    "# The first is the same sales dataset that we worked with in Module 5\n",
    "\n",
    "sales_df = pd.read_pickle('./data/sales_df.pickle')\n",
    "customer_df = pd.read_pickle('./data/customers_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "810xuGeL5kse"
   },
   "outputs": [],
   "source": [
    "# Also the same as Module 5, we'll force Pandas to show all of the columns when printing\n",
    "pd.set_option('display.max_columns', 1000) # Now we will see up to 1000 columns\n",
    "pd.set_option('display.max_colwidth', 1000) # And we will see up to 1000 characters in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipAlec9z6XHm"
   },
   "source": [
    "##########\n",
    "## 1.2 DATA DESCRIPTIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "colab_type": "code",
    "id": "ivXtIy-FvOtG",
    "outputId": "b7760d7b-604f-4549-8080-1405261f5c6c"
   },
   "outputs": [],
   "source": [
    "# We can use the .describe() function to view some general statistics about each column in a dataframe\n",
    "# Let's look at the sales dataframe\n",
    "sales_description = sales_df.describe()\n",
    "sales_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "xPJ26ce_6lkS",
    "outputId": "17504d95-35e2-47d2-820e-54a2169a055b"
   },
   "outputs": [],
   "source": [
    "# Now let's look at the customer dataframe\n",
    "cust_description = customer_df.describe()\n",
    "cust_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "s-2YNfyt6sTG",
    "outputId": "47b0ee7a-0acb-4d7c-cac2-3b00c2715dbd"
   },
   "outputs": [],
   "source": [
    "# To determine the data type of each column, we can use the .dtypes function\n",
    "# Note: Strings will show as 'object'\n",
    "dtypes = sales_df.dtypes\n",
    "dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYr9Zqdj6xeC"
   },
   "source": [
    "##########\n",
    "## 1.3 AGGREGATIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "KDkfhxCw6uo2",
    "outputId": "19c5d220-c9f7-4967-daa6-02c9d6099221"
   },
   "outputs": [],
   "source": [
    "# There are many built-in aggregation function we can use to compute individual column statistics\n",
    "# These include, but are not limited to: count, sum, mean, median, mode, min, max, abs, prod, std, var \n",
    "# Note: Some of these statistics are the same as what we see when we use the .describe() function\n",
    "\n",
    "# We can use .count() to tell us the number of rows in each column containing non-NA values\n",
    "\n",
    "sales_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "oQqYkAiq60hY",
    "outputId": "88aa15e3-f1b9-4ba0-824b-b0747325b72a"
   },
   "outputs": [],
   "source": [
    "# For all of these aggregation functions, we can also apply them to a subset of columns\n",
    "\n",
    "# selection = sales_df[ ['customer_key', 'product_key'] ]\n",
    "# count = selection.count()\n",
    "# print(count)\n",
    "\n",
    "sales_df[['customer_key', 'product_key']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qgpj5v1p61_C",
    "outputId": "843bbbac-a4f9-42f0-c533-ca86d23c3f09"
   },
   "outputs": [],
   "source": [
    "# We can use .sum() to sum all of the values in each column\n",
    "selection = sales_df['product_cost']\n",
    "my_sum = selection.sum() # Note: We cannot call our variable 'sum' because that is a reserved Python keyword\n",
    "print(my_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "WBg28VFJ66PQ",
    "outputId": "df4892b3-2dd5-42c3-8f3d-2c5036739d8b"
   },
   "outputs": [],
   "source": [
    "# We can also use the .agg() function to compute multiple statistics on 1 or more columns at once\n",
    "# Note: This is similar to what the .describe() function does, but it allows us to specify the stats we want\n",
    "stats = sales_df['product_cost'].agg(['count', 'sum', 'mean', 'std', 'min', 'max'])\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "Find the average product_cost, average order_quantity, and mean product_price\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aczcUyhS7cwo"
   },
   "source": [
    "########################\n",
    "# 2. GROUPED AGGREGATIONS\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxY-KWii7edp"
   },
   "source": [
    "##########\n",
    "## 2.1 BASIC GROUPED AGGREGATIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "GS_vDfL3vOtJ",
    "outputId": "bd4c08a5-8a4d-4c5e-ae9f-d38766c3169e"
   },
   "outputs": [],
   "source": [
    "# Rather than computing column statistics that consider each row individually, \n",
    "#   we can group rows based on one of their column values using the .groupby() function\n",
    "# Then, we can compute statistics for each group\n",
    "\n",
    "# We'll start with a simple example: grouping and computing a single metric\n",
    "grouped_data = sales_df.groupby('product_name')\n",
    "count = grouped_data['order_quantity'].agg('count') # Count number of rows in each group with non-NA values for 'OrderQuantity'\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "Using .loc -\n",
    "\n",
    "Find the order_quantity count for 'Classic Vest, M'\n",
    "\n",
    "Hint: DataFrame indices can be found with `df_name.index`\n",
    "'''\n",
    "\n",
    "count.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "iodxQzJg7l0q",
    "outputId": "75891b5f-797e-4f32-b026-75d620f695d2"
   },
   "outputs": [],
   "source": [
    "# We can accomplish the same thing in a more compact way\n",
    "count = sales_df.groupby('product_name')['order_quantity'].agg('count')\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "NKMNXAqT7pvg",
    "outputId": "caf21d42-9d84-40ec-afd5-919d0489b8f8"
   },
   "outputs": [],
   "source": [
    "# Similar to what we saw in the previous section, we can compute multiple metrics at once\n",
    "# Group and compute\n",
    "stats = sales_df.groupby('product_name')['order_quantity'].agg(['count', 'sum', 'mean', 'std', 'min', 'max']) \n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "Using .loc -\n",
    "\n",
    "Find the max order_quantity for 'Bike Wash - Dissolver'\n",
    "\n",
    "Hint: DataFrame columns can be found with `df_name.columns`\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj_drfxK7uev"
   },
   "outputs": [],
   "source": [
    "# We can use .reset_index() to move the 'product_name' from the index back to a regular column,\n",
    "# and set the new row numbers as the index.\n",
    "stats = stats.reset_index()\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "From sales_df\n",
    "1. Find the average product_cost for each product_name.\n",
    "2. Find the sum of product_cost for 'AWC Logo Cap'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsSdesNI7xhP"
   },
   "source": [
    "##########\n",
    "## 2.2 ADVANCED GROUPED AGGREGATIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odoLdftJvOtL"
   },
   "outputs": [],
   "source": [
    "# For more complex aggregations, it's best to use your own aggregation function\n",
    "\n",
    "# Here, we define a function that computes the 90th percentile value for a given data series\n",
    "import numpy as np\n",
    "def ninetieth_percentile(x):\n",
    "  p = np.percentile(x, 90)\n",
    "  return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "bskJp1qp71Jf",
    "outputId": "a1a204da-31f4-4c2b-c450-503be54d4989"
   },
   "outputs": [],
   "source": [
    "# Then, we can supply the function name as an input in the .agg() function\n",
    "stats = sales_df.groupby('category_name')['product_price'].agg(['min', ninetieth_percentile, 'max']).reset_index() \n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WsykvRvO7_Qe"
   },
   "source": [
    "########################\n",
    "# 3. SORTING AND FILTERING\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wcubKu9g8AbZ"
   },
   "source": [
    "##########\n",
    "## 3.1 SORTING\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "6E-SZ7wdvOtO",
    "outputId": "afcb3558-7930-4aa1-8de9-65b1a6086e73",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can sort rows using the .sort_values() function\n",
    "# Note: If sorting using multiple columns, the function first sorts by the left-most column provided\n",
    "#       Then, rows are further sorted by the next column provided, etc.\n",
    "#       This can handle cases where some column values are the same.\n",
    "sorted_df = sales_df.sort_values(by = ['stock_date', 'product_name', 'product_size'], ascending=True) \n",
    "\n",
    "sorted_selection = sorted_df[['stock_date', 'product_name', 'product_size', 'model_name']]\n",
    "\n",
    "sorted_selection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = sorted_df.reset_index(drop = True)\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 994
    },
    "colab_type": "code",
    "id": "bFiAIP2SvOtR",
    "outputId": "6fdc91ad-119c-48b7-d5da-6b7252964e43"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 3.2 FILTERING\n",
    "##########\n",
    "# Filtering rows is the equivalent of using `WHERE` in SQL.\n",
    "# Note: Filtering occurs by requesting rows where a condition is `True`.\n",
    "\n",
    "# First, we need to build a series of True/False values for each row \n",
    "# The value is true for each row where column 'CategoryName' has value 'Bikes'\n",
    "wanted_category_name = 'Bikes' \n",
    "rows_tf = (sales_df['category_name'] == wanted_category_name)\n",
    "rows_tf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we use the True/False series to filter the dataframe\n",
    "# The result of this funtion is a new dataframe with all of the rows that meet our filter condition\n",
    "filtered_df = sales_df[rows_tf]\n",
    "print('The original dataframe has %d rows' % len(sales_df))\n",
    "print('The filtered dataframe has %d rows' % len(filtered_df))\n",
    "\n",
    "filtered_df[['product_name', 'model_name', 'product_style']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "Find all rows with product cost > 2000\n",
    "How many rows are there?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can accomplish the same filtering in a more compact way\n",
    "wanted_category_name = 'Bikes' \n",
    "columns_wanted = ['product_name', 'model_name', 'product_style', 'subcategory_name']\n",
    "filtered_df = sales_df[sales_df['category_name'] == wanted_category_name][columns_wanted]\n",
    "\n",
    "print('The filtered dataframe has %d rows' % len(filtered_df))\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also filter using compound conditions\n",
    "wanted_category_name = 'Bikes' \n",
    "subcategory_name = 'Mountain Bikes'\n",
    "\n",
    "columns_wanted = ['product_name', 'model_name', 'product_style', 'subcategory_name']\n",
    "\n",
    "rows_tf = (sales_df['category_name'] == wanted_category_name) & (sales_df['subcategory_name'] == subcategory_name)\n",
    "\n",
    "filtered_df = sales_df[rows_tf][columns_wanted]\n",
    "\n",
    "print('The filtered dataframe has %d rows' % len(filtered_df))\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another type of filtering is considering only unique values, using the .drop_duplicates() function\n",
    "# Here, we first sort the dataframe using the 'ProductName' and 'ModelName' columns\n",
    "\n",
    "sorted_df = sales_df.sort_values(by = ['product_name', 'model_name'], ascending=False) \n",
    "# print(sorted_df[['product_name', 'model_name', 'product_style']].head())\n",
    "\n",
    "# Then, we drop all rows containing duplicate values in columns 'ProductName' and 'ModelName'\n",
    "unique_sorted_df = sorted_df.drop_duplicates(['product_name', 'model_name'])\n",
    "\n",
    "# Now, let's look at the result\n",
    "unique_sorted_df[['product_name', 'model_name', 'product_style']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sorted_df[['product_name', 'model_name', 'product_style']].reset_index(drop = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "Find all rows with category = Bikes, subcategory_name == Mountain Bikes, and product_cost between 1500 and 2000\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "PF0yWbh4vOtU",
    "outputId": "1d01ab23-e385-4fd5-8949-84cd372f643b"
   },
   "source": [
    "########################\n",
    "# 4. TABLE JOINS\n",
    "########################\n",
    "\n",
    "https://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins <br>\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##########\n",
    "## 4.1 GETTING STARTED\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "sales_df = pd.read_pickle('./data/sales_df.pickle')\n",
    "customer_df = pd.read_pickle('./data/customers_data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Although the sales data (sales_df) has more rows than the customer data (customer_df), \n",
    "# the former has fewer unique customer_key's than the latter.\n",
    "\n",
    "# inner and outer joins\n",
    "\n",
    "# Sales data\n",
    "print(len(sales_df['customer_key']))\n",
    "print(len(sales_df['customer_key'].drop_duplicates()))\n",
    "\n",
    "# Customer data\n",
    "print(len(customer_df['customer_key']))\n",
    "print(len(customer_df['customer_key'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.loc[:5, 'customer_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.loc[:5, 'customer_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(sales_df.columns.to_list())\n",
    "y = np.asarray(customer_df.columns.to_list())\n",
    "\n",
    "np.intersect1d(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 4.2 INNER JOIN\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "eUoWdepcvOtX",
    "outputId": "730d3b53-337a-4237-8b46-1abcc5966523"
   },
   "outputs": [],
   "source": [
    "# Get the intersection of the Sales and customer entries with an identical customer_key\n",
    "\n",
    "joined_df = pd.merge(   left=sales_df, right=customer_df,                 # Left and right dataframes\n",
    "                        how='inner',                                      # Specify inner join\n",
    "                        left_on='customer_key', right_on='customer_key',  # Join keys\n",
    "                        suffixes=('_sales', '_cust')                      # To apply to overlapping column names\n",
    "                    )\n",
    "\n",
    "print('The joined df has %d unique customer_key values' % len(joined_df['customer_key'].drop_duplicates()))\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 4.3 LEFT JOIN\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "MhhfSkn0vOtZ",
    "outputId": "c2c3095b-7214-4a47-f321-9708f8c8d7cd"
   },
   "outputs": [],
   "source": [
    "# \"Complement\" the Sales data with the Customer dataset\n",
    "# Some sales entries do not have a counterpart in the cust data. Look at the `_merge` column added by the \n",
    "# argument 'indicator=True' to see which rows from the sales data did not have a counterpart in the customer dataset\n",
    "joined_df = pd.merge(   left=sales_df, right=customer_df, \n",
    "                        how='left',\n",
    "                        left_on='customer_key', right_on='customer_key',\n",
    "                        suffixes=('_sales', '_cust'),\n",
    "                        indicator=True)\n",
    "print('The joined df has %d unique customer_key values' % len(joined_df['customer_key'].drop_duplicates()))\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 4.4 RIGHT JOIN\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "toKmtPNHvOtc",
    "outputId": "f2b41409-71ff-4558-e345-37456e99b596"
   },
   "outputs": [],
   "source": [
    "# For each entry in the customer dataset, find all the sales which match on customer_key\n",
    "# Look at the _merge column added by the argument 'indicator=True' \n",
    "#  to see which customer rows did not have a counterpart in the sales dataset.\n",
    "joined_df = pd.merge(   left=sales_df, right=customer_df, \n",
    "                        how='right',\n",
    "                        left_on='customer_key', right_on='customer_key',\n",
    "                        suffixes=('_sales', '_cust'),\n",
    "                        indicator=True)\n",
    "print('The joined df has %d unique customer_key values' % len(joined_df['customer_key'].drop_duplicates()))\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "1. Find rows in the df created after right join that are only present in customer_df\n",
    "\n",
    "HINT : _merge can have the values - 'both', 'left_only', 'right_only'\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 4.5 OUTER JOIN\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "7V5D9hcQvOte",
    "outputId": "4d0e09b3-07fc-4936-dd3c-6b011749519e"
   },
   "outputs": [],
   "source": [
    "# Keep all the customer_key's from both sales and customer datasets and attmpt to match each of them (left + right join).\n",
    "# Look at the '_merge' column added by the argument 'indicator=True' to see which rows did not have a counterpart \n",
    "# in the other dataset.\n",
    "joined_df = pd.merge(   left=sales_df, right=customer_df, \n",
    "                        how='outer',\n",
    "                        left_on='customer_key', right_on='customer_key',\n",
    "                        suffixes=('_sales', '_cust'))\n",
    "print('The joined df has %d unique customer_key values' % len(joined_df['customer_key'].drop_duplicates()))\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################\n",
    "# 5. ADVANCED STATISTICAL METHODS\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 5.1 LAMBDA FUNCTIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908
    },
    "colab_type": "code",
    "id": "kOCnqpt4vOtg",
    "outputId": "9403ab30-3a9d-417b-aedf-d3ee9578c9df"
   },
   "outputs": [],
   "source": [
    "# Lambda functions are single-line functions that typically perform some time of data transformation \n",
    "# They can be used inside the .apply() function to very efficiently perform the same operation\n",
    "# on each row in a dataframe column\n",
    "\n",
    "# For example, the following lambda function is used to add 'NEW--' to the beginning of each value in the 'OrderNumber' column\n",
    "# The new values are stored in a new column named 'new_OrderNumber'\n",
    "sales_df['new_order_number'] = sales_df['order_number'].apply(func=(lambda x: 'NEW--' + str(x)))\n",
    "sales_df[['order_number', 'new_order_number']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use lambda functions that act on multiple dataframe columns\n",
    "# Can you tell what the following function will do before executing the code?\n",
    "sales_df['new_order_name_date'] = sales_df.apply(lambda row: ' - '.join([str(row.order_number), str(row.order_date)]), axis=1)\n",
    "sales_df[['order_number', 'order_date', 'new_order_name_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more complex tranformations, its best to create a normal function that is called within the lambda function\n",
    "# Here, we define the function\n",
    "def concatenate_num_date(order_num, order_date):\n",
    "  concatenated = ''.join(['NEW--', str(order_num), '--', str(order_date)])\n",
    "  return(concatenated)\n",
    "\n",
    "# Let's look at what the function does on a single set of data points\n",
    "print(concatenate_num_date('foo', 'bar'))\n",
    "# Now we'll use the function in our lambda function\n",
    "sales_df['new_order_name_date'] = sales_df.apply(lambda row: concatenate_num_date(row.order_number, row.order_date), axis=1)\n",
    "sales_df[['order_number', 'order_date', 'new_order_name_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Progress bar\n",
    "# Use tqdm's progress_apply to render a progress bar during complex / long processing\n",
    "from tqdm import tqdm # Import the function\n",
    "tqdm.pandas() # Start tqdm for Pandas operations\n",
    "\n",
    "# Now, we'll use .progress_apply() instead of the .apply() we have been using\n",
    "sales_df['new_order_name_date'] = sales_df.progress_apply(lambda row: concatenate_num_date(row.order_number, row.order_date), axis=1)\n",
    "sales_df[['order_number', 'order_date', 'new_order_name_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "Create a new row called \"Expensive\" with the values True or False. \n",
    "1. True if the product cost is >= 2000\n",
    "2. False if it is less than 2000\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 5.2 CORRELATIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "AUmrVnFDvOti",
    "outputId": "26550b08-5254-4fd2-f4e2-e636bf07d9a5"
   },
   "outputs": [],
   "source": [
    "# We can use the .corr() function to compute the pairwise correlation of columns, excluding NA/null values\n",
    "# By default, the function uses the standard Pearson correlation coefficient\n",
    "# But we can also choose to use the Kendall Tau coefficient (method=kendall) or the Spearman rank correlation (method=spearman)\n",
    "# We can also define our own correlation method function, and then use method=function_name\n",
    "# To learn more: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
    "\n",
    "# Here, we'll compute the correlation of all columns in the sales dataframe\n",
    "corr_matrix = sales_df.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearly, it doesn't make much sense to compute correlations between every column\n",
    "# So let's build a new dataframe with only a few columns, then compute the column correlation again\n",
    "my_cols = sales_df[['order_quantity', 'product_cost', 'product_price']]\n",
    "corr_matrix = my_cols.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 5.3 MATRIX MULTIPLICATIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ypRSUvzYvOtk",
    "outputId": "16799ca4-19f2-4526-8c0e-ed43388a4f69"
   },
   "outputs": [],
   "source": [
    "# To compute the innner product between two dataframes or series, we can use the .dot() function\n",
    "# Note: The dimensions of DataFrame and other must be compatible in order to compute the matrix multiplication. \n",
    "#       In addition, the column names of DataFrame and the index of other must contain the same values, \n",
    "#       as they will be aligned prior to the multiplication.\n",
    "# Learn more: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dot.html \n",
    "\n",
    "# Here, we show a simple example of computing the dot product between two data series\n",
    "series1 = pd.Series([7, 5, 6, 4, 9]) \n",
    "series2 = pd.Series([1, 2, 3, 10, 2]) \n",
    "dot_prod = series1.dot(series2) \n",
    "dot_prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################\n",
    "# 6. TRANSFORMING DATAFRAMES\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 852
    },
    "colab_type": "code",
    "id": "gzlxgOMFvOtn",
    "outputId": "f30fec0c-e0f5-498f-ac8c-f11bad6acee9"
   },
   "outputs": [],
   "source": [
    "# We can use the .pivot() function to create a new derived dataframe out of a given one \n",
    "# The function takes 3 arguements 'index', 'columns', and 'values' - each must be a column name in the original dataframe\n",
    "# When executed, the function will create a new dataframe, whose row and column indices are the unique values of the \n",
    "# respective parameters. The cell values of the new table are taken from column given as the 'values' parameter.\n",
    "\n",
    "# In this example, we'll be looking at new sales quantity for each brand for each day\n",
    "# First, we'll isolate the columns we need, then aggregate the new sales quantities\n",
    "\n",
    "my_cols = sales_df[['order_date', 'product_name', 'order_quantity']]\n",
    "\n",
    "# new_sales_df = my_cols.groupby(['order_date', 'product_name'], as_index = False).agg('sum')\n",
    "\n",
    "new_sales_df = my_cols.groupby(['order_date', 'product_name']).agg('sum').reset_index()\n",
    "new_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll create a new table with columns = ProductName, rows = OrderDate, and values = OrderQuantity\n",
    "result1 = new_sales_df.pivot(index='order_date', columns='product_name', values='order_quantity').fillna(0).astype(int)\n",
    "result1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .pivot_table() function is a generalization of .pivot() that can handle duplicate values for one pivoted index/column pair \n",
    "# Specifically, you can give .pivot_table() a list of aggregation functions using keyword argument aggfunc\n",
    "# The default aggfunc of .pivot_table() is numpy.mean\n",
    "\n",
    "# Let's do the same operation we just did, but using .pivot_table() this time\n",
    "result2 = sales_df.pivot_table(index='order_date', columns='product_name', values='order_quantity', aggfunc=np.sum, fill_value = 0)\n",
    "result2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm that the two results are the same, we can use the .equals() function\n",
    "equal_bool = result1.equals(result2)\n",
    "print('The two results are equal:', equal_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .transpose() function is fairly self-explanatory - we can use this function to \n",
    "# compute the transpose of a dataframe. Sometimes this comes in handy to get data ready for plotting.\n",
    "# For example, let's say we need our OrderQuantity pivot table to have columns = dates and rows = products\n",
    "transposed1 = result2.transpose()\n",
    "transposed1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use df.T to accomplish the same thing (The property T is an accessor to the .transpose() function)\n",
    "transposed2 = result2.T\n",
    "print('The two results are equal:', transposed1.equals(transposed2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "1. Create a new column called 'revenue' which is the product of 'product_price' and 'order_quantity' in a new df called 'new_sales_df'\n",
    "2. Create a new table with 'order_date' as the index, 'product_name' as column and the value as the aggregate of the revenue values for the product on a date (pivot vs pivot_table)\n",
    "3. Find the total revenue from AWC Logo Cap in 2016\n",
    "'''\n",
    "\n",
    "# new_sales_df = sales_df\n",
    "# new_sales_df['revenue'] = sales_df.apply(lambda x: x['order_quantity'] * x['product_price'], axis = 1)\n",
    "# t = new_sales_df.pivot_table(index = 'order_date', columns = 'product_name', values = 'revenue', aggfunc = np.sum, fill_value = 0)\n",
    "# t\n",
    "# t.loc['2016-01-01' : '2017-12-31', 'AWC Logo Cap'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################\n",
    "# 7. VISUALIZING DATA\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUcMuLy8vOtq"
   },
   "outputs": [],
   "source": [
    "# Pandas dataframes work very well with another Python library - matplotlib\n",
    "# For more information about the matplotlib library, see:\n",
    "# https://matplotlib.org/\n",
    "# https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet\n",
    "\n",
    "# First, we need to import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 7.1 Bar Chart\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZviAgJzvOts",
    "outputId": "ef8c38cc-66e5-4da2-dd8b-4c188de29ea2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this example, display the order quantity for each category\n",
    "\n",
    "# First, we'll prepare the data\n",
    "data_to_plot = sales_df.groupby('category_name')['order_quantity'].agg('count')\n",
    "data_to_plot = data_to_plot.reset_index()\n",
    "data_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we'll make sure our plotting object is cleared\n",
    "plt.clf()\n",
    "\n",
    "# Then, we'll create a plot showing the order quantity for each category\n",
    "ax = data_to_plot.plot.bar(x = 'category_name', y = 'order_quantity')\n",
    "\n",
    "# Then, we'll set the plot title, x and y axis labels\n",
    "ax.set_title('Bar chart showing the order quantity of each category')\n",
    "ax.set_xlabel(\"Category\")\n",
    "ax.set_ylabel(\"Order Quantity\")\n",
    "\n",
    "# Then, we'll save the plot into a directory called 'plots/'\n",
    "plt.savefig('./plots/bar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 7.2 TIME SERIES\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EraKrV_mvOtt",
    "outputId": "142bf18e-adc7-4f53-ed8d-23cfc6c83283"
   },
   "outputs": [],
   "source": [
    "# In this example, we will plot the daily sales for Road Bikes, Mountain Bikes, Touring Bikes over time\n",
    "\n",
    "# First, let's look at the data type of our 'order_date' column (this column contains date information)\n",
    "print(sales_df['order_date'].dtypes) # Result is 'object' meaning the column contains strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to plot the data over time, we need to convert the column from a string to a datetime type,\n",
    "# which can be plotted as a time series. Let's also convert the dates into month bins by setting each\n",
    "# day to the first of the month, in order to make the visualization easier to interpret.\n",
    "\n",
    "# We will use a lambda function to do this :)\n",
    "\n",
    "sales_df['order_date'] = sales_df['order_date'].progress_apply(lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_df['order_date'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_df = sales_df[sales_df['category_name'] == 'Bikes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll keep only the columns we need\n",
    "filtered_df = bikes_df[['order_date', 'subcategory_name', 'order_quantity']]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['subcategory_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll group first by 'order_date', then by 'subcategory_name', \n",
    "# and sum the only remaining column 'order_quantity'\n",
    "total_bikes_df = filtered_df.groupby(['order_date', 'subcategory_name']).agg('sum').reset_index()\n",
    "# total_sales_df = total_sales_df.sort_values(by = ['order_date'])\n",
    "# Let's look at what our data now looks like\n",
    "total_bikes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bikes_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to pivot the dataframe to get it in the right shape for plotting\n",
    "pivoted_sales_df = total_bikes_df.pivot(index='order_date', values='order_quantity', columns='subcategory_name')\n",
    "# Let's look at what our data now looks like\n",
    "pivoted_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_sales_df = pivoted_sales_df.fillna(0)\n",
    "pivoted_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we're ready to plot\n",
    "# We clear the plot area, plot our data on the axis, add labels, and save the figure\n",
    "plt.clf()\n",
    "ax = pivoted_sales_df.plot()\n",
    "ax.set_title('Total weekly sales for Road Bikes, Mountain Bikes, Touring Bikes')\n",
    "ax.set_xlabel(\"Week beginning\")\n",
    "ax.set_ylabel(\"Total sales\")\n",
    "plt.savefig('plots/time_series.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An easier to visualize way to plot this data\n",
    "\n",
    "pivoted_sales_df = pivoted_sales_df.cumsum(axis = 0)\n",
    "pivoted_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we're ready to plot\n",
    "# We clear the plot area, plot our data on the axis, add labels, and save the figure\n",
    "plt.clf()\n",
    "ax = pivoted_sales_df.plot()\n",
    "ax.set_title('Total weekly sales for Road Bikes, Mountain Bikes, Touring Bikes')\n",
    "ax.set_xlabel(\"Week beginning\")\n",
    "ax.set_ylabel(\"Total sales\")\n",
    "plt.savefig('plots/time_series.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 7.3 HEAT MAP\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kn9jlM6JvOty",
    "outputId": "4ca5426b-74b3-4f33-afd5-7e95f60796df"
   },
   "outputs": [],
   "source": [
    "# Another plotting option is a heat map \n",
    "\n",
    "# We import a supporting library and prepare the plot area\n",
    "import seaborn as sns\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20,10)) # This explicity sets the size of the plot area (2x as wide as tall)\n",
    "ax = plt.axes()\n",
    "\n",
    "# Then we plot the data, add labels, and save the figure\n",
    "sns.heatmap(pivoted_sales_df, ax = ax)\n",
    "ax.set_title('Heatmap of weekly sales for Road Bikes, Mountain Bikes, Touring Bikes')\n",
    "ax.set_xlabel(\"Bike Type\")\n",
    "ax.set_ylabel(\"Week beginning\")\n",
    "plt.savefig('plots/heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 7.4 SCATTER PLOT\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HJYVuLBvOt1",
    "outputId": "799e3b71-835d-4f49-a55e-0994f982445e"
   },
   "outputs": [],
   "source": [
    "# Let's plot month vs total sales amount \n",
    "new_sales_df = sales_df\n",
    "new_sales_df['revenue'] = sales_df.apply(lambda x: x['order_quantity'] * x['product_price'], axis = 1)\n",
    "# new_sales_df['revenue'] = sales_df.apply(lambda x: , axis = 1)\n",
    "\n",
    "# Let's see if the month has an effect on the amount of revenue\n",
    "rev = new_sales_df.groupby(new_sales_df.order_date.dt.month).agg('revenue').sum().reset_index()\n",
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(rev.order_date, rev.revenue, marker='o', linestyle='', ms=12, alpha = 0.5)\n",
    "plt.savefig('plots/scatter_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 7.5 SCATTER PLOT WITH NICER FORMATTING\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Matplotlib allows for an extensive amount of formatting customization\n",
    "# Options include, but are certainly not limited to: graph title, axis titles, grid, axis range, legend position\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# And axis (data series) title\n",
    "ax.set_title('Monthly sales overview')\n",
    "# Make the grid visible\n",
    "ax.grid(True)\n",
    "# Set the x axis label and range\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_xlim((0, 15))\n",
    "# Set the y axis label and range\n",
    "ax.set_ylabel('Total sales value')\n",
    "ax.set_ylim((0, 5000000))\n",
    "\n",
    "ax.plot(rev.order_date, rev.revenue, marker='o', linestyle='', ms=12, alpha = 0.5)\n",
    "\n",
    "# Now we'll save the new version and compare to the previous one\n",
    "plt.savefig('plots/scatter_plot_new.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################\n",
    "# 8. ACTIVITIES\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgsfqb6KvOt5"
   },
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv('./data/customers_data.csv')\n",
    "\n",
    "# Let's switch to the customers dataset for the activities. First we have to turn the annual_income\n",
    "# column from a type string into a type integer\n",
    "def salary_transform(string):\n",
    "    s = string.replace('$', '')\n",
    "    s = s.replace(',', '')\n",
    "    return int(s)\n",
    "\n",
    "customer_df['annual_income'] = customer_df['annual_income'].apply(lambda x: salary_transform(x))\n",
    "customer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 8.1 ACTIVITY 1\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the customer_df:\n",
    "# 1. Which 'occupation' has the HIGHEST MEAN and MEDIAN 'annual_income' value? And what are the values? \n",
    "# Hint: it's the same occupation.\n",
    "\n",
    "# 2. For the occupation you found in step 1, how many rows in customer_df does it appear in?\n",
    "\n",
    "###\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "# Correct results for step 1: occupation = Management, mean = 92118.53, median = 90000\n",
    "# Correct results for step 2: 3011 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "## 8.2 ACTIVITY 2\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv('./data/sales_data_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89h9-6crvOt9"
   },
   "outputs": [],
   "source": [
    "# Using the sales_df:\n",
    "# 1. Use a lambda function to compute ('product_price' - 'product_cost') for each row and store the value in \n",
    "#    a new column called 'delta'\n",
    "#    Then, sum all of the resulting values in 'delta' to compare with the correct answer below\n",
    "# \n",
    "# 2. Use a lambda function and subsequent aggregation to compute how many more characters, \n",
    "#    on average (mean), the 'product_description' column has compared to the 'product_name' \n",
    "\n",
    "###\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "# Correct answer for step 1: 10268688.964499999\n",
    "# Correct answer for step 2: 55.49814438140099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "module6_data_analysis_in_pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
